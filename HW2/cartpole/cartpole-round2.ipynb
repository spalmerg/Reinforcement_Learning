{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym.spaces\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 300 # ideally > 1,000\n",
    "gamma = 0.95 # specified by homework\n",
    "\n",
    "# network parameters\n",
    "learning_rate = 0.01\n",
    "hidden_size = 10\n",
    "\n",
    "# epochs \n",
    "epochs = 1000 # will be a lot on GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyGradient():\n",
    "    def __init__(self, learning_rate=0.01, state_size=4, action_size=2, hidden_size=10, name='PolicyGradient'):\n",
    "        with tf.variable_scope(name):\n",
    "            self.inputs_ = tf.placeholder(tf.float32, [None, state_size], name='inputs')\n",
    "            self.actions_ = tf.placeholder(tf.int32, [None, action_size], name='actions')\n",
    "            self.expected_future_rewards_ = tf.placeholder(tf.float32, [None,], name=\"expected_future_rewards\")\n",
    "            \n",
    "            # Hidden Layers\n",
    "            self.fc1 = tf.contrib.layers.fully_connected(self.inputs_, hidden_size, \n",
    "                                                         weights_initializer=tf.contrib.layers.xavier_initializer())\n",
    "            self.fc2 = tf.contrib.layers.fully_connected(self.fc1, action_size, \n",
    "                                                         weights_initializer=tf.contrib.layers.xavier_initializer())\n",
    "            self.fc3 = tf.contrib.layers.fully_connected(self.fc2, action_size,\n",
    "                                                         weights_initializer=tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "            # Output Layer\n",
    "            self.action_distribution = tf.nn.softmax(self.fc3)\n",
    "            \n",
    "            self.log_prob = tf.nn.softmax_cross_entropy_with_logits_v2(logits = self.fc3, labels = self.actions_)\n",
    "            self.loss = tf.reduce_mean(self.log_prob * self.expected_future_rewards_) \n",
    "            \n",
    "            # adjust network\n",
    "            self.learn = tf.train.AdamOptimizer(learning_rate).minimize(self.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "network = PolicyGradient(name = 'pray4us', hidden_size=hidden_size, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up environment\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "# Initialize the simulation\n",
    "env.reset()\n",
    "\n",
    "# Take one random step to get the pole and cart moving\n",
    "state, reward, done, _ = env.step(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Future Reward Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_rewards(episode_rewards):\n",
    "    discounted_episode_rewards = np.zeros_like(episode_rewards)\n",
    "    future = 0.0\n",
    "    for i in reversed(range(len(episode_rewards))):\n",
    "        future = episode_rewards[i] + (gamma * future)\n",
    "        discounted_episode_rewards[i] = future\n",
    "        \n",
    "    mean = np.mean(discounted_episode_rewards)\n",
    "    std = np.std(discounted_episode_rewards)\n",
    "    discounted_episode_rewards = (discounted_episode_rewards - mean) / (std)\n",
    "    \n",
    "    return discounted_episode_rewards.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================\n",
      "Episode:  0\n",
      "Reward:  17.0\n",
      "Mean Reward 17.0\n",
      "==========================================\n",
      "Episode:  1\n",
      "Reward:  30.0\n",
      "Mean Reward 23.5\n",
      "==========================================\n",
      "Episode:  2\n",
      "Reward:  45.0\n",
      "Mean Reward 30.666666666666668\n",
      "==========================================\n",
      "Episode:  3\n",
      "Reward:  57.0\n",
      "Mean Reward 37.25\n",
      "==========================================\n",
      "Episode:  4\n",
      "Reward:  76.0\n",
      "Mean Reward 45.0\n",
      "==========================================\n",
      "Episode:  5\n",
      "Reward:  107.0\n",
      "Mean Reward 55.333333333333336\n",
      "==========================================\n",
      "Episode:  6\n",
      "Reward:  127.0\n",
      "Mean Reward 65.57142857142857\n",
      "==========================================\n",
      "Episode:  7\n",
      "Reward:  177.0\n",
      "Mean Reward 79.5\n",
      "==========================================\n",
      "Episode:  8\n",
      "Reward:  208.0\n",
      "Mean Reward 93.77777777777777\n",
      "==========================================\n",
      "Episode:  9\n",
      "Reward:  234.0\n",
      "Mean Reward 107.8\n",
      "==========================================\n",
      "Episode:  10\n",
      "Reward:  255.0\n",
      "Mean Reward 121.18181818181819\n",
      "==========================================\n",
      "Episode:  11\n",
      "Reward:  278.0\n",
      "Mean Reward 134.25\n",
      "==========================================\n",
      "Episode:  12\n",
      "Reward:  298.0\n",
      "Mean Reward 146.84615384615384\n",
      "==========================================\n",
      "Episode:  13\n",
      "Reward:  308.0\n",
      "Mean Reward 158.35714285714286\n",
      "==========================================\n",
      "Episode:  14\n",
      "Reward:  320.0\n",
      "Mean Reward 169.13333333333333\n",
      "==========================================\n",
      "Episode:  15\n",
      "Reward:  332.0\n",
      "Mean Reward 179.3125\n",
      "==========================================\n",
      "Episode:  16\n",
      "Reward:  365.0\n",
      "Mean Reward 190.23529411764707\n",
      "==========================================\n",
      "Episode:  17\n",
      "Reward:  380.0\n",
      "Mean Reward 200.77777777777777\n",
      "==========================================\n",
      "Episode:  18\n",
      "Reward:  413.0\n",
      "Mean Reward 211.94736842105263\n",
      "==========================================\n",
      "Episode:  19\n",
      "Reward:  459.0\n",
      "Mean Reward 224.3\n",
      "==========================================\n",
      "Episode:  20\n",
      "Reward:  482.0\n",
      "Mean Reward 236.57142857142858\n",
      "==========================================\n",
      "Episode:  21\n",
      "Reward:  524.0\n",
      "Mean Reward 249.63636363636363\n",
      "==========================================\n",
      "Episode:  22\n",
      "Reward:  562.0\n",
      "Mean Reward 263.2173913043478\n",
      "==========================================\n",
      "Episode:  23\n",
      "Reward:  576.0\n",
      "Mean Reward 276.25\n",
      "==========================================\n",
      "Episode:  24\n",
      "Reward:  584.0\n",
      "Mean Reward 288.56\n",
      "==========================================\n",
      "Episode:  25\n",
      "Reward:  597.0\n",
      "Mean Reward 300.4230769230769\n",
      "==========================================\n",
      "Episode:  26\n",
      "Reward:  617.0\n",
      "Mean Reward 312.14814814814815\n",
      "==========================================\n",
      "Episode:  27\n",
      "Reward:  643.0\n",
      "Mean Reward 323.9642857142857\n",
      "==========================================\n",
      "Episode:  28\n",
      "Reward:  690.0\n",
      "Mean Reward 336.58620689655174\n",
      "==========================================\n",
      "Episode:  29\n",
      "Reward:  739.0\n",
      "Mean Reward 350.0\n",
      "==========================================\n",
      "Episode:  30\n",
      "Reward:  771.0\n",
      "Mean Reward 363.5806451612903\n",
      "==========================================\n",
      "Episode:  31\n",
      "Reward:  861.0\n",
      "Mean Reward 379.125\n",
      "==========================================\n",
      "Episode:  32\n",
      "Reward:  893.0\n",
      "Mean Reward 394.6969696969697\n",
      "==========================================\n",
      "Episode:  33\n",
      "Reward:  910.0\n",
      "Mean Reward 409.8529411764706\n",
      "==========================================\n",
      "Episode:  34\n",
      "Reward:  933.0\n",
      "Mean Reward 424.8\n",
      "==========================================\n",
      "Episode:  35\n",
      "Reward:  1010.0\n",
      "Mean Reward 441.05555555555554\n",
      "==========================================\n",
      "Episode:  36\n",
      "Reward:  1037.0\n",
      "Mean Reward 457.1621621621622\n",
      "==========================================\n",
      "Episode:  37\n",
      "Reward:  1052.0\n",
      "Mean Reward 472.8157894736842\n",
      "==========================================\n",
      "Episode:  38\n",
      "Reward:  1063.0\n",
      "Mean Reward 487.94871794871796\n",
      "==========================================\n",
      "Episode:  39\n",
      "Reward:  1082.0\n",
      "Mean Reward 502.8\n",
      "==========================================\n",
      "Episode:  40\n",
      "Reward:  1097.0\n",
      "Mean Reward 517.2926829268292\n",
      "==========================================\n",
      "Episode:  41\n",
      "Reward:  1133.0\n",
      "Mean Reward 531.952380952381\n",
      "==========================================\n",
      "Episode:  42\n",
      "Reward:  1165.0\n",
      "Mean Reward 546.6744186046511\n",
      "==========================================\n",
      "Episode:  43\n",
      "Reward:  1187.0\n",
      "Mean Reward 561.2272727272727\n",
      "==========================================\n",
      "Episode:  44\n",
      "Reward:  1207.0\n",
      "Mean Reward 575.5777777777778\n",
      "==========================================\n",
      "Episode:  45\n",
      "Reward:  1226.0\n",
      "Mean Reward 589.7173913043479\n",
      "==========================================\n",
      "Episode:  46\n",
      "Reward:  1255.0\n",
      "Mean Reward 603.8723404255319\n",
      "==========================================\n",
      "Episode:  47\n",
      "Reward:  1273.0\n",
      "Mean Reward 617.8125\n",
      "==========================================\n",
      "Episode:  48\n",
      "Reward:  1304.0\n",
      "Mean Reward 631.8163265306123\n",
      "==========================================\n",
      "Episode:  49\n",
      "Reward:  1316.0\n",
      "Mean Reward 645.5\n",
      "==========================================\n",
      "Episode:  50\n",
      "Reward:  1333.0\n",
      "Mean Reward 658.9803921568628\n",
      "==========================================\n",
      "Episode:  51\n",
      "Reward:  1343.0\n",
      "Mean Reward 672.1346153846154\n",
      "==========================================\n",
      "Episode:  52\n",
      "Reward:  1373.0\n",
      "Mean Reward 685.3584905660377\n",
      "==========================================\n",
      "Episode:  53\n",
      "Reward:  1403.0\n",
      "Mean Reward 698.6481481481482\n",
      "==========================================\n",
      "Episode:  54\n",
      "Reward:  1427.0\n",
      "Mean Reward 711.8909090909091\n",
      "==========================================\n",
      "Episode:  55\n",
      "Reward:  1440.0\n",
      "Mean Reward 724.8928571428571\n",
      "==========================================\n",
      "Episode:  56\n",
      "Reward:  1461.0\n",
      "Mean Reward 737.8070175438596\n",
      "==========================================\n",
      "Episode:  57\n",
      "Reward:  1484.0\n",
      "Mean Reward 750.6724137931035\n",
      "==========================================\n",
      "Episode:  58\n",
      "Reward:  1500.0\n",
      "Mean Reward 763.3728813559322\n",
      "==========================================\n",
      "Episode:  59\n",
      "Reward:  1545.0\n",
      "Mean Reward 776.4\n",
      "==========================================\n",
      "Episode:  60\n",
      "Reward:  1571.0\n",
      "Mean Reward 789.4262295081967\n",
      "==========================================\n",
      "Episode:  61\n",
      "Reward:  1604.0\n",
      "Mean Reward 802.5645161290323\n",
      "==========================================\n",
      "Episode:  62\n",
      "Reward:  1619.0\n",
      "Mean Reward 815.5238095238095\n",
      "==========================================\n",
      "Episode:  63\n",
      "Reward:  1645.0\n",
      "Mean Reward 828.484375\n",
      "==========================================\n",
      "Episode:  64\n",
      "Reward:  1671.0\n",
      "Mean Reward 841.4461538461538\n",
      "==========================================\n",
      "Episode:  65\n",
      "Reward:  1686.0\n",
      "Mean Reward 854.2424242424242\n",
      "==========================================\n",
      "Episode:  66\n",
      "Reward:  1720.0\n",
      "Mean Reward 867.1641791044776\n",
      "==========================================\n",
      "Episode:  67\n",
      "Reward:  1743.0\n",
      "Mean Reward 880.0441176470588\n",
      "==========================================\n",
      "Episode:  68\n",
      "Reward:  1762.0\n",
      "Mean Reward 892.8260869565217\n",
      "==========================================\n",
      "Episode:  69\n",
      "Reward:  1775.0\n",
      "Mean Reward 905.4285714285714\n",
      "==========================================\n",
      "Episode:  70\n",
      "Reward:  1822.0\n",
      "Mean Reward 918.3380281690141\n",
      "==========================================\n",
      "Episode:  71\n",
      "Reward:  1838.0\n",
      "Mean Reward 931.1111111111111\n",
      "==========================================\n",
      "Episode:  72\n",
      "Reward:  1867.0\n",
      "Mean Reward 943.931506849315\n",
      "==========================================\n",
      "Episode:  73\n",
      "Reward:  1926.0\n",
      "Mean Reward 957.2027027027027\n",
      "==========================================\n",
      "Episode:  74\n",
      "Reward:  2013.0\n",
      "Mean Reward 971.28\n",
      "==========================================\n",
      "Episode:  75\n",
      "Reward:  2026.0\n",
      "Mean Reward 985.1578947368421\n",
      "==========================================\n",
      "Episode:  76\n",
      "Reward:  2045.0\n",
      "Mean Reward 998.922077922078\n",
      "==========================================\n",
      "Episode:  77\n",
      "Reward:  2058.0\n",
      "Mean Reward 1012.5\n",
      "==========================================\n",
      "Episode:  78\n",
      "Reward:  2088.0\n",
      "Mean Reward 1026.113924050633\n",
      "==========================================\n",
      "Episode:  79\n",
      "Reward:  2107.0\n",
      "Mean Reward 1039.625\n",
      "==========================================\n",
      "Episode:  80\n",
      "Reward:  2134.0\n",
      "Mean Reward 1053.1358024691358\n",
      "==========================================\n",
      "Episode:  81\n",
      "Reward:  2156.0\n",
      "Mean Reward 1066.5853658536585\n",
      "==========================================\n",
      "Episode:  82\n",
      "Reward:  2168.0\n",
      "Mean Reward 1079.855421686747\n",
      "==========================================\n",
      "Episode:  83\n",
      "Reward:  2187.0\n",
      "Mean Reward 1093.0357142857142\n",
      "==========================================\n",
      "Episode:  84\n",
      "Reward:  2198.0\n",
      "Mean Reward 1106.035294117647\n",
      "==========================================\n",
      "Episode:  85\n",
      "Reward:  2230.0\n",
      "Mean Reward 1119.1046511627908\n",
      "==========================================\n",
      "Episode:  86\n",
      "Reward:  2255.0\n",
      "Mean Reward 1132.16091954023\n",
      "==========================================\n",
      "Episode:  87\n",
      "Reward:  2284.0\n",
      "Mean Reward 1145.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================\n",
      "Episode:  88\n",
      "Reward:  2306.0\n",
      "Mean Reward 1158.2921348314608\n",
      "==========================================\n",
      "Episode:  89\n",
      "Reward:  2322.0\n",
      "Mean Reward 1171.2222222222222\n",
      "==========================================\n",
      "Episode:  90\n",
      "Reward:  2340.0\n",
      "Mean Reward 1184.065934065934\n",
      "==========================================\n",
      "Episode:  91\n",
      "Reward:  2369.0\n",
      "Mean Reward 1196.945652173913\n",
      "==========================================\n",
      "Episode:  92\n",
      "Reward:  2391.0\n",
      "Mean Reward 1209.784946236559\n",
      "==========================================\n",
      "Episode:  93\n",
      "Reward:  2405.0\n",
      "Mean Reward 1222.5\n",
      "==========================================\n",
      "Episode:  94\n",
      "Reward:  2423.0\n",
      "Mean Reward 1235.1368421052632\n",
      "==========================================\n",
      "Episode:  95\n",
      "Reward:  2435.0\n",
      "Mean Reward 1247.6354166666667\n",
      "==========================================\n",
      "Episode:  96\n",
      "Reward:  2444.0\n",
      "Mean Reward 1259.9690721649486\n",
      "==========================================\n",
      "Episode:  97\n",
      "Reward:  2578.0\n",
      "Mean Reward 1273.4183673469388\n",
      "==========================================\n",
      "Episode:  98\n",
      "Reward:  2602.0\n",
      "Mean Reward 1286.8383838383838\n",
      "==========================================\n",
      "Episode:  99\n",
      "Reward:  2626.0\n",
      "Mean Reward 1300.23\n",
      "==========================================\n",
      "Episode:  100\n",
      "Reward:  2642.0\n",
      "Mean Reward 1313.5148514851485\n",
      "==========================================\n",
      "Episode:  101\n",
      "Reward:  2652.0\n",
      "Mean Reward 1326.637254901961\n",
      "==========================================\n",
      "Episode:  102\n",
      "Reward:  2679.0\n",
      "Mean Reward 1339.7669902912621\n",
      "==========================================\n",
      "Episode:  103\n",
      "Reward:  2724.0\n",
      "Mean Reward 1353.076923076923\n",
      "==========================================\n",
      "Episode:  104\n",
      "Reward:  2773.0\n",
      "Mean Reward 1366.6\n",
      "==========================================\n",
      "Episode:  105\n",
      "Reward:  2799.0\n",
      "Mean Reward 1380.1132075471698\n",
      "==========================================\n",
      "Episode:  106\n",
      "Reward:  2811.0\n",
      "Mean Reward 1393.4859813084113\n",
      "==========================================\n",
      "Episode:  107\n",
      "Reward:  2862.0\n",
      "Mean Reward 1407.0833333333333\n",
      "==========================================\n",
      "Episode:  108\n",
      "Reward:  2878.0\n",
      "Mean Reward 1420.5779816513761\n",
      "==========================================\n",
      "Episode:  109\n",
      "Reward:  2939.0\n",
      "Mean Reward 1434.3818181818183\n",
      "==========================================\n",
      "Episode:  110\n",
      "Reward:  2957.0\n",
      "Mean Reward 1448.099099099099\n",
      "==========================================\n",
      "Episode:  111\n",
      "Reward:  2971.0\n",
      "Mean Reward 1461.6964285714287\n",
      "==========================================\n",
      "Episode:  112\n",
      "Reward:  2986.0\n",
      "Mean Reward 1475.1858407079646\n",
      "==========================================\n",
      "Episode:  113\n",
      "Reward:  2997.0\n",
      "Mean Reward 1488.5350877192982\n",
      "==========================================\n",
      "Episode:  114\n",
      "Reward:  3044.0\n",
      "Mean Reward 1502.0608695652174\n",
      "==========================================\n",
      "Episode:  115\n",
      "Reward:  3058.0\n",
      "Mean Reward 1515.4741379310344\n",
      "==========================================\n",
      "Episode:  116\n",
      "Reward:  3071.0\n",
      "Mean Reward 1528.7692307692307\n",
      "==========================================\n",
      "Episode:  117\n",
      "Reward:  3113.0\n",
      "Mean Reward 1542.1949152542372\n",
      "==========================================\n",
      "Episode:  118\n",
      "Reward:  3159.0\n",
      "Mean Reward 1555.781512605042\n",
      "==========================================\n",
      "Episode:  119\n",
      "Reward:  3177.0\n",
      "Mean Reward 1569.2916666666667\n",
      "==========================================\n",
      "Episode:  120\n",
      "Reward:  3196.0\n",
      "Mean Reward 1582.7355371900826\n",
      "==========================================\n",
      "Episode:  121\n",
      "Reward:  3209.0\n",
      "Mean Reward 1596.0655737704917\n",
      "==========================================\n",
      "Episode:  122\n",
      "Reward:  3235.0\n",
      "Mean Reward 1609.3902439024391\n",
      "==========================================\n",
      "Episode:  123\n",
      "Reward:  3291.0\n",
      "Mean Reward 1622.9516129032259\n",
      "==========================================\n",
      "Episode:  124\n",
      "Reward:  3310.0\n",
      "Mean Reward 1636.448\n",
      "==========================================\n",
      "Episode:  125\n",
      "Reward:  3325.0\n",
      "Mean Reward 1649.8492063492063\n",
      "==========================================\n",
      "Episode:  126\n",
      "Reward:  3373.0\n",
      "Mean Reward 1663.4173228346456\n",
      "==========================================\n",
      "Episode:  127\n",
      "Reward:  3408.0\n",
      "Mean Reward 1677.046875\n",
      "==========================================\n",
      "Episode:  128\n",
      "Reward:  3423.0\n",
      "Mean Reward 1690.5813953488373\n",
      "==========================================\n",
      "Episode:  129\n",
      "Reward:  3435.0\n",
      "Mean Reward 1704.0\n",
      "==========================================\n",
      "Episode:  130\n",
      "Reward:  3470.0\n",
      "Mean Reward 1717.4809160305344\n",
      "==========================================\n",
      "Episode:  131\n",
      "Reward:  3490.0\n",
      "Mean Reward 1730.909090909091\n",
      "==========================================\n",
      "Episode:  132\n",
      "Reward:  3532.0\n",
      "Mean Reward 1744.451127819549\n",
      "==========================================\n",
      "Episode:  133\n",
      "Reward:  3546.0\n",
      "Mean Reward 1757.8955223880596\n",
      "==========================================\n",
      "Episode:  134\n",
      "Reward:  3564.0\n",
      "Mean Reward 1771.2740740740742\n",
      "==========================================\n",
      "Episode:  135\n",
      "Reward:  3602.0\n",
      "Mean Reward 1784.735294117647\n",
      "==========================================\n",
      "Episode:  136\n",
      "Reward:  3654.0\n",
      "Mean Reward 1798.3795620437957\n",
      "==========================================\n",
      "Episode:  137\n",
      "Reward:  3667.0\n",
      "Mean Reward 1811.9202898550725\n",
      "==========================================\n",
      "Episode:  138\n",
      "Reward:  3711.0\n",
      "Mean Reward 1825.5827338129495\n",
      "==========================================\n",
      "Episode:  139\n",
      "Reward:  3724.0\n",
      "Mean Reward 1839.142857142857\n",
      "==========================================\n",
      "Episode:  140\n",
      "Reward:  3747.0\n",
      "Mean Reward 1852.6737588652481\n",
      "==========================================\n",
      "Episode:  141\n",
      "Reward:  3802.0\n",
      "Mean Reward 1866.4014084507041\n",
      "==========================================\n",
      "Episode:  142\n",
      "Reward:  3835.0\n",
      "Mean Reward 1880.167832167832\n",
      "==========================================\n",
      "Episode:  143\n",
      "Reward:  3857.0\n",
      "Mean Reward 1893.8958333333333\n",
      "==========================================\n",
      "Episode:  144\n",
      "Reward:  3883.0\n",
      "Mean Reward 1907.6137931034482\n",
      "==========================================\n",
      "Episode:  145\n",
      "Reward:  3917.0\n",
      "Mean Reward 1921.376712328767\n",
      "==========================================\n",
      "Episode:  146\n",
      "Reward:  3929.0\n",
      "Mean Reward 1935.0340136054422\n",
      "==========================================\n",
      "Episode:  147\n",
      "Reward:  3945.0\n",
      "Mean Reward 1948.6148648648648\n",
      "==========================================\n",
      "Episode:  148\n",
      "Reward:  3978.0\n",
      "Mean Reward 1962.234899328859\n",
      "==========================================\n",
      "Episode:  149\n",
      "Reward:  3997.0\n",
      "Mean Reward 1975.8\n",
      "==========================================\n",
      "Episode:  150\n",
      "Reward:  4022.0\n",
      "Mean Reward 1989.3509933774835\n",
      "==========================================\n",
      "Episode:  151\n",
      "Reward:  4034.0\n",
      "Mean Reward 2002.8026315789473\n",
      "==========================================\n",
      "Episode:  152\n",
      "Reward:  4049.0\n",
      "Mean Reward 2016.1764705882354\n",
      "==========================================\n",
      "Episode:  153\n",
      "Reward:  4063.0\n",
      "Mean Reward 2029.4675324675325\n",
      "==========================================\n",
      "Episode:  154\n",
      "Reward:  4084.0\n",
      "Mean Reward 2042.7225806451613\n",
      "==========================================\n",
      "Episode:  155\n",
      "Reward:  4098.0\n",
      "Mean Reward 2055.897435897436\n",
      "==========================================\n",
      "Episode:  156\n",
      "Reward:  4122.0\n",
      "Mean Reward 2069.0573248407645\n",
      "==========================================\n",
      "Episode:  157\n",
      "Reward:  4134.0\n",
      "Mean Reward 2082.1265822784812\n",
      "==========================================\n",
      "Episode:  158\n",
      "Reward:  4156.0\n",
      "Mean Reward 2095.169811320755\n",
      "==========================================\n",
      "Episode:  159\n",
      "Reward:  4172.0\n",
      "Mean Reward 2108.15\n",
      "==========================================\n",
      "Episode:  160\n",
      "Reward:  4187.0\n",
      "Mean Reward 2121.0621118012423\n",
      "==========================================\n",
      "Episode:  161\n",
      "Reward:  4197.0\n",
      "Mean Reward 2133.8765432098767\n",
      "==========================================\n",
      "Episode:  162\n",
      "Reward:  4212.0\n",
      "Mean Reward 2146.6257668711655\n",
      "==========================================\n",
      "Episode:  163\n",
      "Reward:  4230.0\n",
      "Mean Reward 2159.329268292683\n",
      "==========================================\n",
      "Episode:  164\n",
      "Reward:  4249.0\n",
      "Mean Reward 2171.9939393939394\n",
      "==========================================\n",
      "Episode:  165\n",
      "Reward:  4296.0\n",
      "Mean Reward 2184.789156626506\n",
      "==========================================\n",
      "Episode:  166\n",
      "Reward:  4326.0\n",
      "Mean Reward 2197.6107784431138\n",
      "==========================================\n",
      "Episode:  167\n",
      "Reward:  4344.0\n",
      "Mean Reward 2210.3869047619046\n",
      "==========================================\n",
      "Episode:  168\n",
      "Reward:  4359.0\n",
      "Mean Reward 2223.1005917159764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================\n",
      "Episode:  169\n",
      "Reward:  4392.0\n",
      "Mean Reward 2235.858823529412\n",
      "==========================================\n",
      "Episode:  170\n",
      "Reward:  4413.0\n",
      "Mean Reward 2248.590643274854\n",
      "==========================================\n",
      "Episode:  171\n",
      "Reward:  4443.0\n",
      "Mean Reward 2261.3488372093025\n",
      "==========================================\n",
      "Episode:  172\n",
      "Reward:  4473.0\n",
      "Mean Reward 2274.1329479768788\n",
      "==========================================\n",
      "Episode:  173\n",
      "Reward:  4485.0\n",
      "Mean Reward 2286.8390804597702\n",
      "==========================================\n",
      "Episode:  174\n",
      "Reward:  4512.0\n",
      "Mean Reward 2299.554285714286\n",
      "==========================================\n",
      "Episode:  175\n",
      "Reward:  4535.0\n",
      "Mean Reward 2312.255681818182\n",
      "==========================================\n",
      "Episode:  176\n",
      "Reward:  4548.0\n",
      "Mean Reward 2324.8870056497176\n",
      "==========================================\n",
      "Episode:  177\n",
      "Reward:  4565.0\n",
      "Mean Reward 2337.4719101123596\n",
      "==========================================\n",
      "Episode:  178\n",
      "Reward:  4582.0\n",
      "Mean Reward 2350.0111731843576\n",
      "==========================================\n",
      "Episode:  179\n",
      "Reward:  4604.0\n",
      "Mean Reward 2362.5333333333333\n",
      "==========================================\n",
      "Episode:  180\n",
      "Reward:  4639.0\n",
      "Mean Reward 2375.1104972375692\n",
      "==========================================\n",
      "Episode:  181\n",
      "Reward:  4659.0\n",
      "Mean Reward 2387.6593406593406\n",
      "==========================================\n",
      "Episode:  182\n",
      "Reward:  4672.0\n",
      "Mean Reward 2400.1420765027324\n",
      "==========================================\n",
      "Episode:  183\n",
      "Reward:  4710.0\n",
      "Mean Reward 2412.695652173913\n",
      "==========================================\n",
      "Episode:  184\n",
      "Reward:  4721.0\n",
      "Mean Reward 2425.172972972973\n",
      "==========================================\n",
      "Episode:  185\n",
      "Reward:  4739.0\n",
      "Mean Reward 2437.6129032258063\n",
      "==========================================\n",
      "Episode:  186\n",
      "Reward:  4765.0\n",
      "Mean Reward 2450.0588235294117\n",
      "==========================================\n",
      "Episode:  187\n",
      "Reward:  4834.0\n",
      "Mean Reward 2462.7393617021276\n",
      "==========================================\n",
      "Episode:  188\n",
      "Reward:  4846.0\n",
      "Mean Reward 2475.3492063492063\n",
      "==========================================\n",
      "Episode:  189\n",
      "Reward:  4863.0\n",
      "Mean Reward 2487.915789473684\n",
      "==========================================\n",
      "Episode:  190\n",
      "Reward:  4907.0\n",
      "Mean Reward 2500.581151832461\n",
      "==========================================\n",
      "Episode:  191\n",
      "Reward:  4928.0\n",
      "Mean Reward 2513.2239583333335\n",
      "==========================================\n",
      "Episode:  192\n",
      "Reward:  4958.0\n",
      "Mean Reward 2525.8911917098444\n",
      "==========================================\n",
      "Episode:  193\n",
      "Reward:  4971.0\n",
      "Mean Reward 2538.494845360825\n",
      "==========================================\n",
      "Episode:  194\n",
      "Reward:  5009.0\n",
      "Mean Reward 2551.1641025641024\n",
      "==========================================\n",
      "Episode:  195\n",
      "Reward:  5021.0\n",
      "Mean Reward 2563.765306122449\n",
      "==========================================\n",
      "Episode:  196\n",
      "Reward:  5076.0\n",
      "Mean Reward 2576.5177664974617\n",
      "==========================================\n",
      "Episode:  197\n",
      "Reward:  5089.0\n",
      "Mean Reward 2589.2070707070707\n",
      "==========================================\n",
      "Episode:  198\n",
      "Reward:  5099.0\n",
      "Mean Reward 2601.819095477387\n",
      "==========================================\n",
      "Episode:  199\n",
      "Reward:  5135.0\n",
      "Mean Reward 2614.485\n",
      "==========================================\n",
      "Episode:  200\n",
      "Reward:  5152.0\n",
      "Mean Reward 2627.1094527363184\n",
      "==========================================\n",
      "Episode:  201\n",
      "Reward:  5174.0\n",
      "Mean Reward 2639.7178217821784\n",
      "==========================================\n",
      "Episode:  202\n",
      "Reward:  5193.0\n",
      "Mean Reward 2652.295566502463\n",
      "==========================================\n",
      "Episode:  203\n",
      "Reward:  5226.0\n",
      "Mean Reward 2664.9117647058824\n",
      "==========================================\n",
      "Episode:  204\n",
      "Reward:  5241.0\n",
      "Mean Reward 2677.478048780488\n",
      "==========================================\n",
      "Episode:  205\n",
      "Reward:  5260.0\n",
      "Mean Reward 2690.0145631067962\n",
      "==========================================\n",
      "Episode:  206\n",
      "Reward:  5274.0\n",
      "Mean Reward 2702.4975845410627\n",
      "==========================================\n",
      "Episode:  207\n",
      "Reward:  5292.0\n",
      "Mean Reward 2714.9471153846152\n",
      "==========================================\n",
      "Episode:  208\n",
      "Reward:  5307.0\n",
      "Mean Reward 2727.349282296651\n",
      "==========================================\n",
      "Episode:  209\n",
      "Reward:  5365.0\n",
      "Mean Reward 2739.9095238095238\n",
      "==========================================\n",
      "Episode:  210\n",
      "Reward:  5380.0\n",
      "Mean Reward 2752.4218009478673\n",
      "==========================================\n",
      "Episode:  211\n",
      "Reward:  5419.0\n",
      "Mean Reward 2765.0\n",
      "==========================================\n",
      "Episode:  212\n",
      "Reward:  5436.0\n",
      "Mean Reward 2777.5399061032863\n",
      "==========================================\n",
      "Episode:  213\n",
      "Reward:  5492.0\n",
      "Mean Reward 2790.2242990654204\n",
      "==========================================\n",
      "Episode:  214\n",
      "Reward:  5509.0\n",
      "Mean Reward 2802.8697674418604\n",
      "==========================================\n",
      "Episode:  215\n",
      "Reward:  5523.0\n",
      "Mean Reward 2815.462962962963\n",
      "==========================================\n",
      "Episode:  216\n",
      "Reward:  5542.0\n",
      "Mean Reward 2828.0276497695854\n",
      "==========================================\n",
      "Episode:  217\n",
      "Reward:  5555.0\n",
      "Mean Reward 2840.5366972477063\n",
      "==========================================\n",
      "Episode:  218\n",
      "Reward:  5571.0\n",
      "Mean Reward 2853.0045662100456\n",
      "==========================================\n",
      "Episode:  219\n",
      "Reward:  5597.0\n",
      "Mean Reward 2865.4772727272725\n",
      "==========================================\n",
      "Episode:  220\n",
      "Reward:  5629.0\n",
      "Mean Reward 2877.981900452489\n",
      "==========================================\n",
      "Episode:  221\n",
      "Reward:  5654.0\n",
      "Mean Reward 2890.4864864864867\n",
      "==========================================\n",
      "Episode:  222\n",
      "Reward:  5691.0\n",
      "Mean Reward 2903.0448430493275\n",
      "==========================================\n",
      "Episode:  223\n",
      "Reward:  5710.0\n",
      "Mean Reward 2915.5758928571427\n",
      "==========================================\n",
      "Episode:  224\n",
      "Reward:  5758.0\n",
      "Mean Reward 2928.208888888889\n",
      "==========================================\n",
      "Episode:  225\n",
      "Reward:  5802.0\n",
      "Mean Reward 2940.924778761062\n",
      "==========================================\n",
      "Episode:  226\n",
      "Reward:  5820.0\n",
      "Mean Reward 2953.6079295154186\n",
      "==========================================\n",
      "Episode:  227\n",
      "Reward:  5838.0\n",
      "Mean Reward 2966.2587719298244\n",
      "==========================================\n",
      "Episode:  228\n",
      "Reward:  5847.0\n",
      "Mean Reward 2978.838427947598\n",
      "==========================================\n",
      "Episode:  229\n",
      "Reward:  5866.0\n",
      "Mean Reward 2991.391304347826\n",
      "==========================================\n",
      "Episode:  230\n",
      "Reward:  5904.0\n",
      "Mean Reward 3004.0\n",
      "==========================================\n",
      "Episode:  231\n",
      "Reward:  5921.0\n",
      "Mean Reward 3016.573275862069\n",
      "==========================================\n",
      "Episode:  232\n",
      "Reward:  5978.0\n",
      "Mean Reward 3029.283261802575\n",
      "==========================================\n",
      "Episode:  233\n",
      "Reward:  6020.0\n",
      "Mean Reward 3042.0641025641025\n",
      "==========================================\n",
      "Episode:  234\n",
      "Reward:  6040.0\n",
      "Mean Reward 3054.8212765957446\n",
      "==========================================\n",
      "Episode:  235\n",
      "Reward:  6057.0\n",
      "Mean Reward 3067.5423728813557\n",
      "==========================================\n",
      "Episode:  236\n",
      "Reward:  6072.0\n",
      "Mean Reward 3080.2194092827003\n",
      "==========================================\n",
      "Episode:  237\n",
      "Reward:  6114.0\n",
      "Mean Reward 3092.9663865546217\n",
      "==========================================\n",
      "Episode:  238\n",
      "Reward:  6128.0\n",
      "Mean Reward 3105.665271966527\n",
      "==========================================\n",
      "Episode:  239\n",
      "Reward:  6144.0\n",
      "Mean Reward 3118.325\n",
      "==========================================\n",
      "Episode:  240\n",
      "Reward:  6163.0\n",
      "Mean Reward 3130.9585062240662\n",
      "==========================================\n",
      "Episode:  241\n",
      "Reward:  6176.0\n",
      "Mean Reward 3143.5413223140495\n",
      "==========================================\n",
      "Episode:  242\n",
      "Reward:  6205.0\n",
      "Mean Reward 3156.1399176954733\n",
      "==========================================\n",
      "Episode:  243\n",
      "Reward:  6232.0\n",
      "Mean Reward 3168.745901639344\n",
      "==========================================\n",
      "Episode:  244\n",
      "Reward:  6275.0\n",
      "Mean Reward 3181.4244897959184\n",
      "==========================================\n",
      "Episode:  245\n",
      "Reward:  6323.0\n",
      "Mean Reward 3194.1951219512193\n",
      "==========================================\n",
      "Episode:  246\n",
      "Reward:  6341.0\n",
      "Mean Reward 3206.935222672065\n",
      "==========================================\n",
      "Episode:  247\n",
      "Reward:  6370.0\n",
      "Mean Reward 3219.689516129032\n",
      "==========================================\n",
      "Episode:  248\n",
      "Reward:  6430.0\n",
      "Mean Reward 3232.582329317269\n",
      "==========================================\n",
      "Episode:  249\n",
      "Reward:  6443.0\n",
      "Mean Reward 3245.424\n",
      "==========================================\n",
      "Episode:  250\n",
      "Reward:  6453.0\n",
      "Mean Reward 3258.203187250996\n",
      "==========================================\n",
      "Episode:  251\n",
      "Reward:  6466.0\n",
      "Mean Reward 3270.9325396825398\n",
      "==========================================\n",
      "Episode:  252\n",
      "Reward:  6478.0\n",
      "Mean Reward 3283.608695652174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================\n",
      "Episode:  253\n",
      "Reward:  6538.0\n",
      "Mean Reward 3296.4212598425197\n",
      "==========================================\n",
      "Episode:  254\n",
      "Reward:  6607.0\n",
      "Mean Reward 3309.4039215686275\n",
      "==========================================\n",
      "Episode:  255\n",
      "Reward:  6621.0\n",
      "Mean Reward 3322.33984375\n",
      "==========================================\n",
      "Episode:  256\n",
      "Reward:  6638.0\n",
      "Mean Reward 3335.241245136187\n",
      "==========================================\n",
      "Episode:  257\n",
      "Reward:  6648.0\n",
      "Mean Reward 3348.0813953488373\n",
      "==========================================\n",
      "Episode:  258\n",
      "Reward:  6709.0\n",
      "Mean Reward 3361.057915057915\n",
      "==========================================\n",
      "Episode:  259\n",
      "Reward:  6729.0\n",
      "Mean Reward 3374.0115384615383\n",
      "==========================================\n",
      "Episode:  260\n",
      "Reward:  6788.0\n",
      "Mean Reward 3387.0919540229884\n",
      "==========================================\n",
      "Episode:  261\n",
      "Reward:  6830.0\n",
      "Mean Reward 3400.232824427481\n",
      "==========================================\n",
      "Episode:  262\n",
      "Reward:  6858.0\n",
      "Mean Reward 3413.380228136882\n",
      "==========================================\n",
      "Episode:  263\n",
      "Reward:  6874.0\n",
      "Mean Reward 3426.4886363636365\n",
      "==========================================\n",
      "Episode:  264\n",
      "Reward:  6894.0\n",
      "Mean Reward 3439.5735849056605\n",
      "==========================================\n",
      "Episode:  265\n",
      "Reward:  6909.0\n",
      "Mean Reward 3452.6165413533836\n",
      "==========================================\n",
      "Episode:  266\n",
      "Reward:  6934.0\n",
      "Mean Reward 3465.6554307116103\n",
      "==========================================\n",
      "Episode:  267\n",
      "Reward:  6960.0\n",
      "Mean Reward 3478.694029850746\n",
      "==========================================\n",
      "Episode:  268\n",
      "Reward:  6969.0\n",
      "Mean Reward 3491.6691449814125\n",
      "==========================================\n",
      "Episode:  269\n",
      "Reward:  6983.0\n",
      "Mean Reward 3504.6\n",
      "==========================================\n",
      "Episode:  270\n",
      "Reward:  7018.0\n",
      "Mean Reward 3517.5645756457566\n",
      "==========================================\n",
      "Episode:  271\n",
      "Reward:  7034.0\n",
      "Mean Reward 3530.4926470588234\n",
      "==========================================\n",
      "Episode:  272\n",
      "Reward:  7058.0\n",
      "Mean Reward 3543.4139194139193\n",
      "==========================================\n",
      "Episode:  273\n",
      "Reward:  7081.0\n",
      "Mean Reward 3556.324817518248\n",
      "==========================================\n",
      "Episode:  274\n",
      "Reward:  7106.0\n",
      "Mean Reward 3569.232727272727\n",
      "==========================================\n",
      "Episode:  275\n",
      "Reward:  7160.0\n",
      "Mean Reward 3582.2427536231885\n",
      "==========================================\n",
      "Episode:  276\n",
      "Reward:  7169.0\n",
      "Mean Reward 3595.191335740072\n",
      "==========================================\n",
      "Episode:  277\n",
      "Reward:  7184.0\n",
      "Mean Reward 3608.1007194244603\n",
      "==========================================\n",
      "Episode:  278\n",
      "Reward:  7216.0\n",
      "Mean Reward 3621.032258064516\n",
      "==========================================\n",
      "Episode:  279\n",
      "Reward:  7272.0\n",
      "Mean Reward 3634.0714285714284\n",
      "==========================================\n",
      "Episode:  280\n",
      "Reward:  7290.0\n",
      "Mean Reward 3647.081850533808\n",
      "==========================================\n",
      "Episode:  281\n",
      "Reward:  7315.0\n",
      "Mean Reward 3660.0886524822695\n",
      "==========================================\n",
      "Episode:  282\n",
      "Reward:  7439.0\n",
      "Mean Reward 3673.441696113074\n",
      "==========================================\n",
      "Episode:  283\n",
      "Reward:  7451.0\n",
      "Mean Reward 3686.742957746479\n",
      "==========================================\n",
      "Episode:  284\n",
      "Reward:  7474.0\n",
      "Mean Reward 3700.0315789473684\n",
      "==========================================\n",
      "Episode:  285\n",
      "Reward:  7506.0\n",
      "Mean Reward 3713.3391608391607\n",
      "==========================================\n",
      "Episode:  286\n",
      "Reward:  7555.0\n",
      "Mean Reward 3726.724738675958\n",
      "==========================================\n",
      "Episode:  287\n",
      "Reward:  7595.0\n",
      "Mean Reward 3740.15625\n",
      "==========================================\n",
      "Episode:  288\n",
      "Reward:  7607.0\n",
      "Mean Reward 3753.536332179931\n",
      "==========================================\n",
      "Episode:  289\n",
      "Reward:  7620.0\n",
      "Mean Reward 3766.8689655172416\n",
      "==========================================\n",
      "Episode:  290\n",
      "Reward:  7633.0\n",
      "Mean Reward 3780.1546391752577\n",
      "==========================================\n",
      "Episode:  291\n",
      "Reward:  7696.0\n",
      "Mean Reward 3793.5650684931506\n",
      "==========================================\n",
      "Episode:  292\n",
      "Reward:  7736.0\n",
      "Mean Reward 3807.0204778156995\n",
      "==========================================\n",
      "Episode:  293\n",
      "Reward:  7749.0\n",
      "Mean Reward 3820.4285714285716\n",
      "==========================================\n",
      "Episode:  294\n",
      "Reward:  7761.0\n",
      "Mean Reward 3833.7864406779663\n",
      "==========================================\n",
      "Episode:  295\n",
      "Reward:  7801.0\n",
      "Mean Reward 3847.189189189189\n",
      "==========================================\n",
      "Episode:  296\n",
      "Reward:  7835.0\n",
      "Mean Reward 3860.6161616161617\n",
      "==========================================\n",
      "Episode:  297\n",
      "Reward:  7856.0\n",
      "Mean Reward 3874.0234899328857\n",
      "==========================================\n",
      "Episode:  298\n",
      "Reward:  7876.0\n",
      "Mean Reward 3887.408026755853\n",
      "==========================================\n",
      "Episode:  299\n",
      "Reward:  7892.0\n",
      "Mean Reward 3900.7566666666667\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-1dea3b14f757>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                 \u001b[0;31m# select action w.r.t. distribution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_prob_dist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction_prob_dist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m                 \u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "all_loss = []\n",
    "all_rewards = []\n",
    "\n",
    "\n",
    "with tf.Session() as sess: \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # save all states, actions, and rewards that happen \n",
    "        episode_states, episode_actions, episode_rewards = [], [], []\n",
    "        \n",
    "        for i_episode in range(episodes):\n",
    "            state = env.reset()\n",
    "            \n",
    "            while True: \n",
    "            \n",
    "                # get action prob distribution w.r.t. policy\n",
    "                feed = {network.inputs_: state.reshape((1,*state.shape))}\n",
    "                action_prob_dist = sess.run(network.action_distribution, feed_dict=feed)\n",
    "                \n",
    "                # select action w.r.t. distribution\n",
    "                action = np.random.choice(range(action_prob_dist.shape[1]), p=action_prob_dist.ravel())\n",
    "                new_state, reward, done, info = env.step(action)\n",
    "                \n",
    "                # keep track of all states, actions, and rewards\n",
    "                episode_states.append(state)\n",
    "                episode_rewards.append(reward)\n",
    "                \n",
    "                \n",
    "                # reformat action for softmax\n",
    "                action_ = np.zeros(action_prob_dist.shape[1])\n",
    "                action_[action] = 1\n",
    "                episode_actions.append(action_)\n",
    "                \n",
    "                # reset current state to be new state\n",
    "                state = new_state\n",
    "                \n",
    "                if done:\n",
    "                    # Calculate sum reward\n",
    "                    episode_rewards_sum = np.sum(episode_rewards)\n",
    "                    all_rewards.append(episode_rewards_sum)\n",
    "                    total_rewards = np.sum(all_rewards)\n",
    "\n",
    "                    # Mean reward\n",
    "                    mean_reward = np.divide(total_rewards, i_episode+1)\n",
    "                    maximumRewardRecorded = np.amax(all_rewards)\n",
    "                    \n",
    "                    if (epoch % 10 == 0):\n",
    "                        print(\"==========================================\")\n",
    "                        print(\"Episode: \", i_episode)\n",
    "                        print(\"Mean Reward\", mean_reward)\n",
    "\n",
    "                    # Calculate discounted reward\n",
    "                    exp_rewards = expected_rewards(episode_rewards)\n",
    "\n",
    "                    # update the network\n",
    "                    loss_, _ = sess.run([network.loss, network.learn], feed_dict={network.inputs_: np.vstack(np.array(episode_states)),\n",
    "                                                                     network.actions_: np.vstack(np.array(episode_actions)),\n",
    "                                                                     network.expected_future_rewards_: exp_rewards \n",
    "                                                                    })\n",
    "                    break\n",
    "        \n",
    "        if (epoch % 100 == 0):\n",
    "            saver.save(sess, \"checkpoints/cartpole{0}.ckpt\".format(epoch))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
