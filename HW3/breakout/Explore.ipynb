{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym.spaces\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from collections import deque\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "discount_rate = 0.95\n",
    "learning_rate = 0.01\n",
    "hidden_size = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(image):\n",
    "    \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 2D float array \"\"\"\n",
    "    image = image[35:195] # crop\n",
    "    image = image[::2,::2,0] # downsample by factor of 2\n",
    "    image[image == 144] = 0 # erase background (background type 1)\n",
    "    image[image == 109] = 0 # erase background (background type 2)\n",
    "    image[image != 0] = 1 # everything else just set to 1\n",
    "    return np.reshape(image.astype(np.float).ravel(), [80,80])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment Reset\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Breakout-v0')\n",
    "env.reset()\n",
    "print('Environment Reset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimator Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network():\n",
    "    def __init__(self, learning_rate=0.01, hidden_size=10, action_size = 4, memory_size = 3, name=\"QEstimator\"):\n",
    "        with tf.variable_scope(name):\n",
    "            # Set scope for copying purposes\n",
    "            self.scope = name\n",
    "\n",
    "            # Store Variables\n",
    "            self.inputs_ = tf.placeholder(tf.float32, [None, memory_size, 80, 80], name='inputs')\n",
    "            self.target_preds_ = tf.placeholder(tf.float32, [None,], name=\"expected_future_rewards\")\n",
    "            self.chosen_action_pred = tf.placeholder(tf.float32, [None,], name=\"chosen_action_pred\")\n",
    "            self.actions_ = tf.placeholder(tf.int32, shape=[None], name='actions')\n",
    "            self.avg_reward_ = tf.placeholder(tf.float32, name=\"avg_reward\")\n",
    "            \n",
    "            # Three Convolutional Layers\n",
    "            self.conv1 = tf.contrib.layers.conv2d(self.inputs_, 32, 8, 4, activation_fn=tf.nn.relu)\n",
    "            self.conv2 = tf.contrib.layers.conv2d(self.conv1, 64, 4, 2, activation_fn=tf.nn.relu)\n",
    "            self.conv3 = tf.contrib.layers.conv2d(self.conv2, 64, 3, 1, activation_fn=tf.nn.relu)\n",
    "\n",
    "            # Fully Connected Layers\n",
    "            self.flatten = tf.contrib.layers.flatten(self.conv3)\n",
    "            self.fc1 = tf.contrib.layers.fully_connected(self.flatten, hidden_size)\n",
    "            self.predictions = tf.contrib.layers.fully_connected(self.fc1, action_size,\n",
    "                                                                 weights_initializer=tf.contrib.layers.xavier_initializer(), \n",
    "                                                                 activation_fn=None)\n",
    "            \n",
    "            # Get Prediction for the chosen action (epsilon greedy)\n",
    "            self.indices = tf.range(1) * tf.size(self.actions_) + self.actions_\n",
    "            self.chosen_action_pred = tf.gather(tf.reshape(self.predictions, [-1]), self.indices)\n",
    "            \n",
    "            # Calculate Loss\n",
    "            # self.losses = tf.squared_difference(self.target_preds_, self.chosen_action_pred)\n",
    "            self.losses = tf.losses.huber_loss(self.target_preds_, self.chosen_action_pred)\n",
    "            self.loss = tf.reduce_mean(self.losses)\n",
    "            \n",
    "            # Adjust Network\n",
    "            self.learn = tf.train.AdamOptimizer(learning_rate).minimize(self.losses)\n",
    "\n",
    "            # For Tensorboard\n",
    "            with tf.name_scope(\"summaries\"):\n",
    "                tf.summary.scalar(\"loss\", self.loss)\n",
    "                tf.summary.scalar(\"avg_epoch_reward\", self.avg_reward_)\n",
    "                self.summary_op = tf.summary.merge_all()\n",
    "            \n",
    "    def predict(self, sess, state):\n",
    "        result = sess.run(self.predictions, feed_dict={self.inputs_: state})\n",
    "        return result\n",
    "    \n",
    "    def update(self, sess, state, action, target_preds, avg_reward):\n",
    "        feed_dict = {self.inputs_: state, \n",
    "                    self.actions_: action, \n",
    "                    self.target_preds_: target_preds,\n",
    "                    self.avg_reward_: avg_reward}\n",
    "        loss = sess.run([self.loss, self.learn, self.summary_op], feed_dict=feed_dict)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Greedy Epsilon Policy\n",
    "Epsilon greedy policy chooses a random action a small fraction of the time. This epsilon can change throughout training, but this is more advanced.... so hold off on that for now @Self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy(sess, network, state, epsilon=0.99):\n",
    "    state = np.stack([state[0], state[1], state[2]])\n",
    "    state = np.expand_dims(state, axis=0)\n",
    "    pick = np.random.rand() # Uniform random number generator\n",
    "    if pick > epsilon: # If off policy -- random action\n",
    "        action = np.random.randint(0,4)\n",
    "    else: # If on policy\n",
    "        action = np.argmax(network.predict(sess, state))\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Parameter Copier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_parameters(sess, q_network, target_network):\n",
    "    \n",
    "    # Get and sort parameters\n",
    "    q_params = [t for t in tf.trainable_variables() if t.name.startswith(q_network.scope)]\n",
    "    q_params = sorted(q_params, key=lambda v: v.name)\n",
    "    t_params = [t for t in tf.trainable_variables() if t.name.startswith(target_network.scope)]\n",
    "    t_params = sorted(t_params, key=lambda v: v.name)\n",
    "    \n",
    "    # Assign Q-Parameters to Target Network\n",
    "    updates = []\n",
    "    for q_v, t_v in zip(q_params, t_params):\n",
    "        update = t_v.assign(q_v)\n",
    "        updates.append(update)\n",
    "    \n",
    "    sess.run(updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sarahgreenwood/anaconda/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "QNetwork = Network(name = 'QNetwork', hidden_size=hidden_size, learning_rate=learning_rate)\n",
    "target = Network(name = 'Target', hidden_size=hidden_size, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-c1ddad9a142b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0;31m# Update Q-Network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mavg_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQNetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_Ts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_reward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;31m# save target network parameters every epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-47-6c731c2dd59a>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, sess, state, action, target_preds, avg_reward)\u001b[0m\n\u001b[1;32m     51\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_preds_\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtarget_preds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                     self.avg_reward_: avg_reward}\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    885\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 887\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    888\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1108\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1110\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1111\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1284\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1286\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1287\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1290\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1292\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1293\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1275\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1277\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1365\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1366\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# saver = tf.train.Saver()\n",
    "buffer_size = 500\n",
    "batch_size = 50\n",
    "epochs = 10\n",
    "reset_every = 100\n",
    "state_memory_size = 3\n",
    "epsilon_start = 0.1\n",
    "epsilon_end = 0.99\n",
    "\n",
    "buffer = deque(maxlen=buffer_size)\n",
    "state_memory = deque(maxlen=state_memory_size)\n",
    "all_result = []\n",
    "\n",
    "with tf.Session() as sess: \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Set up count for network reset\n",
    "    count = 0\n",
    "    \n",
    "    # Make epsilon greedy schedule\n",
    "    epsilons = np.linspace(epsilon_start, epsilon_end, epochs)\n",
    "    \n",
    "    # Set up memory for episode\n",
    "    start_state = preprocess(env.reset())\n",
    "    for fill in range(state_memory_size):\n",
    "        state_memory.append(start_state)\n",
    "    \n",
    "    # Fill The Buffer\n",
    "    for i in range(buffer_size):\n",
    "        action = epsilon_greedy(sess, QNetwork, state_memory)\n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        # Save old_state_memory\n",
    "        old_state_memory = state_memory\n",
    "        \n",
    "        # Add new_state to state_memory\n",
    "        state_memory.append(preprocess(new_state))\n",
    "        \n",
    "        # Add step to buffer\n",
    "        buffer.append([old_state_memory, action, state_memory, reward])\n",
    "        \n",
    "        # If done, reset memory\n",
    "        if done: \n",
    "            start_state = preprocess(env.reset())\n",
    "            for fill in range(state_memory_size):\n",
    "                state_memory.append(start_state)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # Set Up Memory\n",
    "        start_state = preprocess(env.reset())\n",
    "        for fill in range(state_memory_size):\n",
    "            state_memory.append(start_state)\n",
    "\n",
    "        result = []\n",
    "        while True: \n",
    "            # Add M to buffer (following policy)\n",
    "            action = epsilon_greedy(sess, QNetwork, state_memory, epsilons[epoch])\n",
    "            new_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            # Save old_state_memory\n",
    "            old_state_memory = state_memory\n",
    "            \n",
    "            # Add new_state to state_memory \n",
    "            state_memory.append(preprocess(new_state))\n",
    "            \n",
    "            # Add step to buffer\n",
    "            buffer.append([old_state_memory, action, state_memory, reward])\n",
    "            \n",
    "            # If simulation done, stop\n",
    "            if done:\n",
    "                break\n",
    "            \n",
    "            ### Sample & update\n",
    "            sample = random.sample(buffer, batch_size)\n",
    "            state_b, action_b, new_state_b, reward_b = map(np.array, zip(*sample))\n",
    "\n",
    "            # Find max Q-Value per batch for progress\n",
    "            Q_preds = sess.run(QNetwork.chosen_action_pred, \n",
    "                                 feed_dict={QNetwork.inputs_: state_b,\n",
    "                                 QNetwork.actions_: action_b})\n",
    "            result.append(np.max(Q_preds))\n",
    "        \n",
    "            # Target Network Predictions + Discount\n",
    "            TPredictions = target.predict(sess, new_state_b)\n",
    "            max_Qt = discount_rate * np.max(TPredictions, axis=1)\n",
    "            action_Ts = reward_b + max_Qt\n",
    "\n",
    "            # Update Q-Network\n",
    "            avg_reward = np.mean(result)\n",
    "            loss, _, summary = QNetwork.update(sess, state_b, action_b, action_Ts, avg_reward)\n",
    "            \n",
    "            # save target network parameters every epoch\n",
    "            count += 1\n",
    "            if count % reset_every == 0:\n",
    "                copy_parameters(sess, QNetwork, target)\n",
    "        \n",
    "        all_result.append(np.mean(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1308cd588>]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x130bdc860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(all_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
